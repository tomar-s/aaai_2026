{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e430610-168c-4b48-b7da-a79da1aca1c6",
   "metadata": {},
   "source": [
    "# Time-Series Search\n",
    "\n",
    "This notebook demonstrates the basic usage of a pre-trained `TSPulse` model for time-series search.\n",
    "For more comprehensive experiments, please refer to the [TSPulse paper](https://arxiv.org/abs/2505.13033).\n",
    "\n",
    "The motivation behind time-series search is to understand and interpret a time series of interest by comparing it with others that exhibit similar patterns.\n",
    "For example, suppose a sudden spike or dip is detected in a machine's sensor reading.\n",
    "Retrieving similar time series from past observations can help to interpret the periodicity of such spikes, identify co-occurring anomalies in other sensors, and more.\n",
    "\n",
    "When searching for similar time series, the following factors are important to consider:\n",
    "- Similar patterns may appear at different positions, with different amplitudes, or slightly shifted in time.\n",
    "- Since time-series data can be very long, it is important to consider overlaps between small segments in the query and those in past data.\n",
    "- Noise and distortions in observations can make matching more difficult.\n",
    "\n",
    "TSPulse addresses these challenges and enables rapid search thanks to its tiny model size and compact embeddings.\n",
    "The embeddings can capture similar patterns in a way that is robust to shifts, scaling, and noiseâ€”enabling powerful zero-shot similarity search without the need for labeled examples.\n",
    "\n",
    "As a simple example of time-series search, we use a classification dataset, treating class labels as the ground truth.\n",
    "The rationale is that time series belonging to the same class may share similar patterns.\n",
    "Each time series has a length of 512 data points (context length). The index database contains 240-dimensional embeddings of 512-length time-series data.\n",
    "Once the query time-series data is provided, we search for similar embeddings using the Euclidean distances.\n",
    "In the final cell, we visualize the query and the retrieved time series to demonstrate the accuracy of TSPulse's search performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7098b-6e6d-40ad-9187-902ddc889b09",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e747a2-e81b-4fe6-b168-c755648b3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "\n",
    "from tsfm_public.models.tspulse import TSPulseForReconstruction\n",
    "from tsfm_public.toolkit.dataset import ClassificationDFDataset\n",
    "from tsfm_public.toolkit.time_series_classification_preprocessor import (\n",
    "    TimeSeriesClassificationPreprocessor,\n",
    ")\n",
    "from tsfm_public.toolkit.util import convert_tsfile_to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160f600-9cdd-43a9-a3cc-656c20283972",
   "metadata": {},
   "source": [
    "## Important arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9c532-14be-4767-b11e-8d3c2daec897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "CONTEXT_LENGTH = 512\n",
    "\n",
    "model_path = \"\"  # modify it to use TSPulse model for search\n",
    "batch_size = 128\n",
    "\n",
    "# search\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e11238b-e3f8-43a0-98cc-4970933d8f75",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d05c18e1-bf7d-49c8-9ba5-cf41d6f35940",
   "metadata": {},
   "source": [
    "# Run the following if the data is not yet downloaded\n",
    "!curl -OL http://www.timeseriesclassification.com/aeon-toolkit/Archives/Univariate2018_ts.zip\n",
    "!unzip Univariate2018_ts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d210bbe-7064-4904-b956-20309f733b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(split):\n",
    "    df_base = convert_tsfile_to_dataframe(\n",
    "        f\"Univariate_ts/ShapesAll/ShapesAll_{split}.ts\",\n",
    "        return_separate_X_and_y=False,\n",
    "    )\n",
    "    label_column = \"class_vals\"\n",
    "    input_columns = [f\"dim_{i}\" for i in range(df_base.shape[1] - 1)]\n",
    "\n",
    "    tsp = TimeSeriesClassificationPreprocessor(\n",
    "        input_columns=input_columns,\n",
    "        label_column=label_column,\n",
    "        scaling=True,\n",
    "    )\n",
    "\n",
    "    tsp.train(df_base)\n",
    "    df_prep = tsp.preprocess(df_base)\n",
    "    base_dataset = ClassificationDFDataset(\n",
    "        df_prep,\n",
    "        id_columns=[],\n",
    "        timestamp_column=None,\n",
    "        input_columns=input_columns,\n",
    "        label_column=label_column,\n",
    "        context_length=CONTEXT_LENGTH,\n",
    "        static_categorical_columns=[],\n",
    "        stride=1,\n",
    "        enable_padding=False,\n",
    "        full_series=True,\n",
    "    )\n",
    "    return base_dataset\n",
    "\n",
    "\n",
    "train_dataset = get_data(\"TRAIN\")\n",
    "test_dataset = get_data(\"TEST\")\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4ecce-0b6d-464e-a50b-fd3f2c9605a0",
   "metadata": {},
   "source": [
    "## Pre-trained Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4403e54-591a-4ce9-8ad5-f5a04ca892d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSPulseForReconstruction.from_pretrained(model_path, num_input_channels=1, mask_ratio=0)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c89ff-396c-4f15-9037-f6ca83a25175",
   "metadata": {},
   "source": [
    "## Index Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc897ae-2c8b-4157-bfbf-4dc8cd2e11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_embedding(batch, model):\n",
    "    # similarity search uses register embeddings\n",
    "    d_model = model.config.d_model\n",
    "    reg_tokens = model.config.patch_register_tokens\n",
    "    reg_emb_size = reg_tokens * d_model\n",
    "\n",
    "    _x = batch[\"past_values\"]  # [B, T, C]\n",
    "    # instance-wise scaling\n",
    "    mu = torch.mean(_x, dim=1, keepdims=True)  # [B, 1, C]\n",
    "    std = torch.std(_x, dim=1, keepdims=True, correction=0)  # [B, 1, C]\n",
    "    x = (_x - mu) / std\n",
    "    embedding = model(x)\n",
    "    embedding = embedding[\"decoder_hidden_state\"]  # [B, T, C] -> [B, C, D]\n",
    "    embedding = embedding[:, :, -reg_emb_size:]\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# compute embeddings for train dataset\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "embeddings = []\n",
    "for batch_idx, batch in enumerate(tqdm(dataloader, total=len(dataloader))):\n",
    "    embedding = get_embedding(batch, model)\n",
    "    embeddings.append(embedding.numpy())\n",
    "train_embeddings = np.concatenate(embeddings)\n",
    "print(train_embeddings.shape)\n",
    "\n",
    "# create index set of embeddings\n",
    "embs = train_embeddings.squeeze(axis=1)  # [B, 1, D] because of the univariate time-series\n",
    "nn = NearestNeighbors(metric=\"l2\")\n",
    "nn.fit(embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479998b-ee80-42d3-86b6-c86eba5a7d1b",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0fcaa-4f2e-4d61-aa4b-e7c62fd17d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision_k(cmp, k):\n",
    "    cmp_k = np.sum(cmp[:, :k], axis=1) / k\n",
    "    mean_cmp_k = np.mean(cmp_k)\n",
    "    return mean_cmp_k\n",
    "\n",
    "\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "cmp, I_all = [], []\n",
    "for batch in tqdm(dataloader):\n",
    "    test_embedding = get_embedding(batch, model)\n",
    "    D, I = nn.kneighbors(test_embedding.squeeze(axis=1), n_neighbors=k)\n",
    "    I_all.append(I)\n",
    "\n",
    "    label_test = batch[\"target_values\"].numpy()\n",
    "    retrieved_label = np.array([[train_dataset[i][\"target_values\"] for i in _I] for _I in I])\n",
    "    cmp.append(label_test[:, None] == retrieved_label)\n",
    "\n",
    "I_all = np.concatenate(I_all, axis=0)  # for visualization\n",
    "cmp = np.concatenate(cmp, axis=0)\n",
    "prec_k = calc_precision_k(cmp, k)\n",
    "print(f\"PREC@{k}={prec_k:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a1221-7bc2-4b2e-9967-23411732bb61",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f7f19-ffc2-4eb8-a45f-e5fb65b384b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotted_list = []\n",
    "for sample_idx, sample in enumerate(test_dataset):\n",
    "    if sample[\"target_values\"] in plotted_list:\n",
    "        continue\n",
    "    elif len(plotted_list) >= 5:  # plot max five classes\n",
    "        break\n",
    "    else:\n",
    "        plotted_list.append(sample[\"target_values\"])\n",
    "\n",
    "    first_match = I_all[sample_idx][0]\n",
    "    second_match = I_all[sample_idx][1]\n",
    "    third_match = I_all[sample_idx][2]\n",
    "\n",
    "    label = test_dataset[sample_idx][\"target_values\"]\n",
    "    first_label = train_dataset[first_match][\"target_values\"]\n",
    "    second_label = train_dataset[second_match][\"target_values\"]\n",
    "    third_label = train_dataset[third_match][\"target_values\"]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(6.4 * 3, 3.4))\n",
    "    ax[0].plot(test_dataset[sample_idx][\"past_values\"])\n",
    "    ax[0].set_title(f\"Query (Test ID: {sample_idx}, class: {label})\")\n",
    "\n",
    "    ax[1].plot(train_dataset[first_match][\"past_values\"])\n",
    "    ax[1].set_title(f\"First match (Train ID: {first_match}, class: {first_label})\")\n",
    "    ax[1].set_facecolor(\"lightgreen\") if label == first_label else None\n",
    "\n",
    "    ax[2].plot(train_dataset[second_match][\"past_values\"])\n",
    "    ax[2].set_title(f\"Second match (Train ID: {second_match}, class: {second_label})\")\n",
    "    ax[2].set_facecolor(\"lightgreen\") if label == second_label else None\n",
    "\n",
    "    ax[3].plot(train_dataset[third_match][\"past_values\"])\n",
    "    ax[3].set_title(f\"Third match (Train ID: {third_match}, class: {third_label})\")\n",
    "    ax[3].set_facecolor(\"lightgreen\") if label == third_label else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949bfa0-02ae-4397-b7d4-a3ba4c2d583f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
