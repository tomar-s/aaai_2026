{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TTM zero-shot and few-shot benchmarking on multiple datasets\n",
    "\n",
    " **Using TTM-1024-96-v1 model.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 13:42:57.255646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Standard\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Third Party\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "# Local\n",
    "from tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\n",
    "from tsfm_public.models.tinytimemixer.utils import (\n",
    "    count_parameters,\n",
    "    get_data,\n",
    "    plot_preds,\n",
    ")\n",
    "\n",
    "# First Party\n",
    "from tsfm_public.toolkit.callbacks import TrackingCallback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "set_seed(42)\n",
    "\n",
    "# Specify model parameters\n",
    "context_length = 1024\n",
    "forecast_length = 96\n",
    "freeze_backbone = True\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Other args\n",
    "EPOCHS = 50\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# Make sure all the datasets in the following `list_datasets` are\n",
    "# saved in the `DATA_ROOT_PATH` folder. Or, change it accordingly.\n",
    "# Refer to the get_data() function\n",
    "# in notebooks/hfdemo/tinytimemixer/utils/ttm_utils.py\n",
    "# to see how it is used.\n",
    "DATA_ROOT_PATH = \"datasets/\"\n",
    "\n",
    "# This is where results will be saved\n",
    "OUT_DIR = \"ttm_results_benchmark_1024_96_tmp\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of benchmark datasets (TTM was not pre-trained on any of these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datasets = [\n",
    "    \"etth1\",\n",
    "    \"etth2\",\n",
    "    \"ettm1\",\n",
    "    \"ettm2\",\n",
    "    \"weather\",\n",
    "    \"electricity\",\n",
    "    \"traffic\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_path = \"ibm/TTM\"\n",
    "if context_length == 512:\n",
    "    hf_model_branch = \"main\"\n",
    "elif context_length == 1024:\n",
    "    hf_model_branch = \"1024_96_v1\"\n",
    "else:\n",
    "    raise ValueError(\"Current supported context lengths are 512 and 1024. Stay tuned for more TTMs!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main benchmarking loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    \"dataset\": [],\n",
    "    \"zs_mse\": [],\n",
    "    \"fs5_mse\": [],\n",
    "    \"fs10_mse\": [],\n",
    "    \"zs_eval_time\": [],\n",
    "    \"fs5_mean_epoch_time\": [],\n",
    "    \"fs5_total_train_time\": [],\n",
    "    \"fs10_mean_epoch_time\": [],\n",
    "    \"fs10_total_train_time\": [],\n",
    "    \"fs5_best_val_metric\": [],\n",
    "    \"fs10_best_val_metric\": [],\n",
    "}\n",
    "# Loop over data\n",
    "for DATASET in list_datasets:\n",
    "    print()\n",
    "    print(\"=\" * 100)\n",
    "    print(\n",
    "        f\"Running zero-shot/few-shot for TTM-{context_length} on dataset = {DATASET}, forecast_len = {forecast_length}\"\n",
    "    )\n",
    "    print(f\"Model will be loaded from {hf_model_path}/{hf_model_branch}\")\n",
    "    SUBDIR = f\"{OUT_DIR}/{DATASET}\"\n",
    "\n",
    "    # Set batch size\n",
    "    if DATASET == \"traffic\":\n",
    "        BATCH_SIZE = 8\n",
    "    elif DATASET == \"electricity\":\n",
    "        BATCH_SIZE = 32\n",
    "    else:\n",
    "        BATCH_SIZE = 64\n",
    "\n",
    "    # Data prep: Get dataset\n",
    "    _, _, dset_test = get_data(DATASET, context_length, forecast_length, data_root_path=DATA_ROOT_PATH)\n",
    "\n",
    "    #############################################################\n",
    "    ##### Use the pretrained model in zero-shot forecasting #####\n",
    "    #############################################################\n",
    "    # Load model\n",
    "    zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(hf_model_path, revision=hf_model_branch)\n",
    "\n",
    "    # zeroshot_trainer\n",
    "    zeroshot_trainer = Trainer(\n",
    "        model=zeroshot_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=f\"{SUBDIR}/zeroshot\",\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "        ),\n",
    "        eval_dataset=dset_test,\n",
    "    )\n",
    "\n",
    "    # evaluate = zero-shot performance\n",
    "    print(\"+\" * 20, \"Test MSE zero-shot\", \"+\" * 20)\n",
    "    zeroshot_output = zeroshot_trainer.evaluate(dset_test)\n",
    "    print(zeroshot_output)\n",
    "    print(\"+\" * 60)\n",
    "    all_results[\"zs_eval_time\"].append(zeroshot_output[\"eval_runtime\"])\n",
    "\n",
    "    # Plot\n",
    "    plot_preds(\n",
    "        zeroshot_trainer,\n",
    "        dset_test,\n",
    "        SUBDIR,\n",
    "        num_plots=10,\n",
    "        plot_prefix=\"test_zeroshot\",\n",
    "        channel=0,\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # write results\n",
    "    all_results[\"dataset\"].append(DATASET)\n",
    "    all_results[\"zs_mse\"].append(zeroshot_output[\"eval_loss\"])\n",
    "\n",
    "    ################################################################\n",
    "    ## Use the pretrained model in few-shot 5% and 10% forecasting #\n",
    "    ################################################################\n",
    "    for fewshot_percent in [5, 10]:\n",
    "        print(\"-\" * 20, f\"Running few-shot {fewshot_percent}%\", \"-\" * 20)\n",
    "        # Data prep: Get dataset\n",
    "        dset_train, dset_val, dset_test = get_data(\n",
    "            DATASET,\n",
    "            context_length,\n",
    "            forecast_length,\n",
    "            fewshot_fraction=fewshot_percent / 100,\n",
    "            data_root_path=DATA_ROOT_PATH,\n",
    "        )\n",
    "\n",
    "        # change head dropout to 0.7 for ett datasets\n",
    "        if \"ett\" in DATASET:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                hf_model_path, revision=hf_model_branch, head_dropout=0.7\n",
    "            )\n",
    "        else:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                hf_model_path, revision=hf_model_branch\n",
    "            )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            print(\n",
    "                \"Number of params before freezing backbone\",\n",
    "                count_parameters(finetune_forecast_model),\n",
    "            )\n",
    "\n",
    "            # Freeze the backbone of the model\n",
    "            for param in finetune_forecast_model.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Count params\n",
    "            print(\n",
    "                \"Number of params after freezing the backbone\",\n",
    "                count_parameters(finetune_forecast_model),\n",
    "            )\n",
    "\n",
    "        print(f\"Using learning rate = {learning_rate}\")\n",
    "        finetune_forecast_args = TrainingArguments(\n",
    "            output_dir=f\"{SUBDIR}/fewshot_{fewshot_percent}\",\n",
    "            overwrite_output_dir=True,\n",
    "            learning_rate=learning_rate,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            do_eval=True,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            dataloader_num_workers=NUM_WORKERS,\n",
    "            report_to=None,\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            save_total_limit=1,\n",
    "            logging_dir=f\"{SUBDIR}/fewshot_{fewshot_percent}\",  # Make sure to specify a logging directory\n",
    "            load_best_model_at_end=True,  # Load the best model when training ends\n",
    "            metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "            greater_is_better=False,  # For loss\n",
    "        )\n",
    "\n",
    "        # Create the early stopping callback\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "            early_stopping_threshold=0.0,  # Minimum improvement required to consider as improvement\n",
    "        )\n",
    "        tracking_callback = TrackingCallback()\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            learning_rate,\n",
    "            epochs=EPOCHS,\n",
    "            steps_per_epoch=math.ceil(len(dset_train) / (BATCH_SIZE)),\n",
    "        )\n",
    "\n",
    "        finetune_forecast_trainer = Trainer(\n",
    "            model=finetune_forecast_model,\n",
    "            args=finetune_forecast_args,\n",
    "            train_dataset=dset_train,\n",
    "            eval_dataset=dset_val,\n",
    "            callbacks=[early_stopping_callback, tracking_callback],\n",
    "            optimizers=(optimizer, scheduler),\n",
    "        )\n",
    "\n",
    "        # Fine tune\n",
    "        finetune_forecast_trainer.train()\n",
    "\n",
    "        # Evaluation\n",
    "        print(\n",
    "            \"+\" * 20,\n",
    "            f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\",\n",
    "            \"+\" * 20,\n",
    "        )\n",
    "        fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
    "        print(fewshot_output)\n",
    "        print(\"+\" * 60)\n",
    "\n",
    "        # Plot\n",
    "        plot_preds(\n",
    "            finetune_forecast_trainer,\n",
    "            dset_test,\n",
    "            SUBDIR,\n",
    "            num_plots=10,\n",
    "            plot_prefix=f\"test_fewshot_{fewshot_percent}\",\n",
    "            channel=0,\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        # write results\n",
    "        all_results[f\"fs{fewshot_percent}_mse\"].append(fewshot_output[\"eval_loss\"])\n",
    "        all_results[f\"fs{fewshot_percent}_mean_epoch_time\"].append(tracking_callback.mean_epoch_time)\n",
    "        all_results[f\"fs{fewshot_percent}_total_train_time\"].append(tracking_callback.total_train_time)\n",
    "        all_results[f\"fs{fewshot_percent}_best_val_metric\"].append(tracking_callback.best_eval_metric)\n",
    "\n",
    "    df_out = pd.DataFrame(all_results).round(3)\n",
    "    print(df_out[[\"dataset\", \"zs_mse\", \"fs5_mse\", \"fs10_mse\"]])\n",
    "    df_out.to_csv(f\"{OUT_DIR}/results_zero_few.csv\")\n",
    "    df_out.to_csv(f\"{OUT_DIR}/results_zero_few.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>zs_mse</th>\n",
       "      <th>fs5_mse</th>\n",
       "      <th>fs10_mse</th>\n",
       "      <th>zs_eval_time</th>\n",
       "      <th>fs5_mean_epoch_time</th>\n",
       "      <th>fs5_total_train_time</th>\n",
       "      <th>fs10_mean_epoch_time</th>\n",
       "      <th>fs10_total_train_time</th>\n",
       "      <th>fs5_best_val_metric</th>\n",
       "      <th>fs10_best_val_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etth1</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.363</td>\n",
       "      <td>4.982</td>\n",
       "      <td>1.032</td>\n",
       "      <td>38.533</td>\n",
       "      <td>1.009</td>\n",
       "      <td>31.517</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>etth2</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.935</td>\n",
       "      <td>26.790</td>\n",
       "      <td>1.028</td>\n",
       "      <td>27.575</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ettm1</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.360</td>\n",
       "      <td>2.012</td>\n",
       "      <td>1.268</td>\n",
       "      <td>50.618</td>\n",
       "      <td>1.659</td>\n",
       "      <td>56.423</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ettm2</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.172</td>\n",
       "      <td>1.905</td>\n",
       "      <td>1.282</td>\n",
       "      <td>38.194</td>\n",
       "      <td>1.675</td>\n",
       "      <td>42.355</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weather</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.150</td>\n",
       "      <td>3.417</td>\n",
       "      <td>1.619</td>\n",
       "      <td>42.032</td>\n",
       "      <td>2.313</td>\n",
       "      <td>49.691</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>electricity</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.139</td>\n",
       "      <td>15.869</td>\n",
       "      <td>4.097</td>\n",
       "      <td>537.848</td>\n",
       "      <td>7.450</td>\n",
       "      <td>440.682</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>traffic</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.424</td>\n",
       "      <td>26.355</td>\n",
       "      <td>6.167</td>\n",
       "      <td>303.630</td>\n",
       "      <td>12.259</td>\n",
       "      <td>406.904</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  zs_mse  fs5_mse  fs10_mse  zs_eval_time  fs5_mean_epoch_time  \\\n",
       "0        etth1   0.362    0.359     0.363         4.982                1.032   \n",
       "1        etth2   0.281    0.280     0.281         0.479                0.935   \n",
       "2        ettm1   0.387    0.372     0.360         2.012                1.268   \n",
       "3        ettm2   0.175    0.173     0.172         1.905                1.282   \n",
       "4      weather   0.152    0.151     0.150         3.417                1.619   \n",
       "5  electricity   0.156    0.145     0.139        15.869                4.097   \n",
       "6      traffic   0.458    0.409     0.424        26.355                6.167   \n",
       "\n",
       "   fs5_total_train_time  fs10_mean_epoch_time  fs10_total_train_time  \\\n",
       "0                38.533                 1.009                 31.517   \n",
       "1                26.790                 1.028                 27.575   \n",
       "2                50.618                 1.659                 56.423   \n",
       "3                38.194                 1.675                 42.355   \n",
       "4                42.032                 2.313                 49.691   \n",
       "5               537.848                 7.450                440.682   \n",
       "6               303.630                12.259                406.904   \n",
       "\n",
       "   fs5_best_val_metric  fs10_best_val_metric  \n",
       "0                0.658                 0.663  \n",
       "1                0.224                 0.223  \n",
       "2                0.409                 0.406  \n",
       "3                0.121                 0.121  \n",
       "4                0.419                 0.425  \n",
       "5                0.112                 0.110  \n",
       "6                0.344                 0.347  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with recent pre-trained model\n",
    "\n",
    "The following tables compare the TTM-1024-96 model's performance with the recently released [MOIRAI model](https://arxiv.org/abs/2402.02592)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing forecasting error (MSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TTM_vs_MOIRAI](images/ttm_moirai.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing model size (in Millions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Number of parameters | MOIRAI size over TTM | \n",
    "|:-----:|:--------------------:|:--------------------:|\n",
    "| TTM-1024-96-v1 | 0.9 M | - |\n",
    "| MOIRAI_Small | 14 M | **16X** |\n",
    "| MOIRAI_Base | 91 M | **101X** |\n",
    "| MOIRAI_Large | 311 M | **346X** |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTM stands out as a pioneering set of compact pre-trained models for time series forecasting. As demonstrated in the preceding results, TTM exhibits a remarkable ability to outperform the recently released MOIRAI model by a substantial margin. Additionally, TTM is significantly more compact, ranging from 16 to 346 times smaller than MOIRAI, showcasing its efficiency and effectiveness in comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
