{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TTM zero-shot and few-shot benchmarking on multiple datasets\n",
    "\n",
    "  **Using TTM-512-96 model.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import TinyTimeMixerForPrediction, TrackingCallback, count_parameters, load_dataset, plot_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "set_seed(42)\n",
    "\n",
    "# Specify model parameters\n",
    "context_length = 512\n",
    "forecast_length = 96\n",
    "freeze_backbone = True\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Other args\n",
    "EPOCHS = 50\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# Make sure all the datasets in the following `list_datasets` are\n",
    "# saved in the `DATA_ROOT_PATH` folder. Or, change it accordingly.\n",
    "# Refer to the get_data() function\n",
    "# in notebooks/hfdemo/tinytimemixer/utils/ttm_utils.py\n",
    "# to see how it is used.\n",
    "DATA_ROOT_PATH = \"datasets/\"\n",
    "\n",
    "# This is where results will be saved\n",
    "OUT_DIR = \"ttm_results_benchmark_512_96/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of benchmark datasets (TTM was not pre-trained on any of these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datasets = [\n",
    "    \"etth1\",\n",
    "    \"etth2\",\n",
    "    \"ettm1\",\n",
    "    \"ettm2\",\n",
    "    \"weather\",\n",
    "    \"electricity\",\n",
    "    \"traffic\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_path = \"ibm/TTM\"\n",
    "if context_length == 512:\n",
    "    hf_model_branch = \"main\"\n",
    "elif context_length == 1024:\n",
    "    hf_model_branch = \"1024_96_v1\"\n",
    "else:\n",
    "    raise ValueError(\"Current supported context lengths are 512 and 1024. Stay tuned for more TTMs!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main benchmarking loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Running zero-shot/few-shot for TTM-512 on dataset = etth1, forecast_len = 96\n",
      "Model will be loaded from ibm/TTM/main\n",
      "etth1 512 96\n",
      "Data lengths: train = 8033, val = 2785, test = 2785\n",
      "++++++++++++++++++++ Test MSE zero-shot ++++++++++++++++++++\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82abfe284a749f79144aa614219fefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36317431926727295, 'eval_runtime': 1.3033, 'eval_samples_per_second': 2136.926, 'eval_steps_per_second': 33.761}\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "-------------------- Running few-shot 5% --------------------\n",
      "etth1 512 96\n",
      "Data lengths: train = 311, val = 2785, test = 2785\n",
      "Number of params before freezing backbone 805280\n",
      "Number of params after freezing the backbone 289696\n",
      "Using learning rate = 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wmgifford/miniconda3/envs/tsfm_public/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/wmgifford/miniconda3/envs/tsfm_public/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae7dec9260248269d7cf430502d4625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 160\u001b[0m\n\u001b[1;32m    150\u001b[0m finetune_forecast_trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    151\u001b[0m     model\u001b[38;5;241m=\u001b[39mfinetune_forecast_model,\n\u001b[1;32m    152\u001b[0m     args\u001b[38;5;241m=\u001b[39mfinetune_forecast_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39m(optimizer, scheduler),\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Fine tune\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mfinetune_forecast_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest MSE after few-shot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfewshot_percent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% fine-tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    167\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/site-packages/transformers/trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/site-packages/accelerate/data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1318\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1318\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1445\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/tsfm_public/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_results = {\n",
    "    \"dataset\": [],\n",
    "    \"zs_mse\": [],\n",
    "    \"fs5_mse\": [],\n",
    "    \"fs10_mse\": [],\n",
    "    \"zs_eval_time\": [],\n",
    "    \"fs5_mean_epoch_time\": [],\n",
    "    \"fs5_total_train_time\": [],\n",
    "    \"fs10_mean_epoch_time\": [],\n",
    "    \"fs10_total_train_time\": [],\n",
    "    \"fs5_best_val_metric\": [],\n",
    "    \"fs10_best_val_metric\": [],\n",
    "}\n",
    "# Loop over data\n",
    "for DATASET in list_datasets:\n",
    "    print()\n",
    "    print(\"=\" * 100)\n",
    "    print(\n",
    "        f\"Running zero-shot/few-shot for TTM-{context_length} on dataset = {DATASET}, forecast_len = {forecast_length}\"\n",
    "    )\n",
    "    print(f\"Model will be loaded from {hf_model_path}/{hf_model_branch}\")\n",
    "    SUBDIR = f\"{OUT_DIR}/{DATASET}\"\n",
    "\n",
    "    # Set batch size\n",
    "    if DATASET == \"traffic\":\n",
    "        BATCH_SIZE = 8\n",
    "    elif DATASET == \"electricity\":\n",
    "        BATCH_SIZE = 32\n",
    "    else:\n",
    "        BATCH_SIZE = 64\n",
    "\n",
    "    # Data prep: Get dataset\n",
    "    _, _, dset_test = load_dataset(DATASET, context_length, forecast_length, dataset_root_path=DATA_ROOT_PATH)\n",
    "\n",
    "    #############################################################\n",
    "    ##### Use the pretrained model in zero-shot forecasting #####\n",
    "    #############################################################\n",
    "    # Load model\n",
    "    zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(hf_model_path, revision=hf_model_branch)\n",
    "\n",
    "    # zeroshot_trainer\n",
    "    zeroshot_trainer = Trainer(\n",
    "        model=zeroshot_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=f\"{SUBDIR}/zeroshot\",\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "        ),\n",
    "        eval_dataset=dset_test,\n",
    "    )\n",
    "\n",
    "    # evaluate = zero-shot performance\n",
    "    print(\"+\" * 20, \"Test MSE zero-shot\", \"+\" * 20)\n",
    "    zeroshot_output = zeroshot_trainer.evaluate(dset_test)\n",
    "    print(zeroshot_output)\n",
    "    print(\"+\" * 60)\n",
    "    all_results[\"zs_eval_time\"].append(zeroshot_output[\"eval_runtime\"])\n",
    "\n",
    "    # Plot\n",
    "    plot_predictions(\n",
    "        zeroshot_trainer.model,\n",
    "        dset_test,\n",
    "        SUBDIR,\n",
    "        num_plots=10,\n",
    "        plot_prefix=\"test_zeroshot\",\n",
    "        channel=0,\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # write results\n",
    "    all_results[\"dataset\"].append(DATASET)\n",
    "    all_results[\"zs_mse\"].append(zeroshot_output[\"eval_loss\"])\n",
    "\n",
    "    ################################################################\n",
    "    ## Use the pretrained model in few-shot 5% and 10% forecasting #\n",
    "    ################################################################\n",
    "    for fewshot_percent in [5, 10]:\n",
    "        print(\"-\" * 20, f\"Running few-shot {fewshot_percent}%\", \"-\" * 20)\n",
    "        # Data prep: Get dataset\n",
    "        dset_train, dset_val, dset_test = load_dataset(\n",
    "            DATASET,\n",
    "            context_length,\n",
    "            forecast_length,\n",
    "            fewshot_fraction=fewshot_percent / 100,\n",
    "            dataset_root_path=DATA_ROOT_PATH,\n",
    "        )\n",
    "\n",
    "        # change head dropout to 0.7 for ett datasets\n",
    "        if \"ett\" in DATASET:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                hf_model_path, revision=hf_model_branch, head_dropout=0.7\n",
    "            )\n",
    "        else:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                hf_model_path, revision=hf_model_branch\n",
    "            )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            print(\n",
    "                \"Number of params before freezing backbone\",\n",
    "                count_parameters(finetune_forecast_model),\n",
    "            )\n",
    "\n",
    "            # Freeze the backbone of the model\n",
    "            for param in finetune_forecast_model.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Count params\n",
    "            print(\n",
    "                \"Number of params after freezing the backbone\",\n",
    "                count_parameters(finetune_forecast_model),\n",
    "            )\n",
    "\n",
    "        print(f\"Using learning rate = {learning_rate}\")\n",
    "        finetune_forecast_args = TrainingArguments(\n",
    "            output_dir=f\"{SUBDIR}/fewshot_{fewshot_percent}\",\n",
    "            overwrite_output_dir=True,\n",
    "            learning_rate=learning_rate,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            do_eval=True,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            dataloader_num_workers=NUM_WORKERS,\n",
    "            report_to=None,\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            save_total_limit=1,\n",
    "            logging_dir=f\"{SUBDIR}/fewshot_{fewshot_percent}\",  # Make sure to specify a logging directory\n",
    "            load_best_model_at_end=True,  # Load the best model when training ends\n",
    "            metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "            greater_is_better=False,  # For loss\n",
    "        )\n",
    "\n",
    "        # Create the early stopping callback\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "            early_stopping_threshold=0.0,  # Minimum improvement required to consider as improvement\n",
    "        )\n",
    "        tracking_callback = TrackingCallback()\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            learning_rate,\n",
    "            epochs=EPOCHS,\n",
    "            steps_per_epoch=math.ceil(len(dset_train) / (BATCH_SIZE)),\n",
    "        )\n",
    "\n",
    "        finetune_forecast_trainer = Trainer(\n",
    "            model=finetune_forecast_model,\n",
    "            args=finetune_forecast_args,\n",
    "            train_dataset=dset_train,\n",
    "            eval_dataset=dset_val,\n",
    "            callbacks=[early_stopping_callback, tracking_callback],\n",
    "            optimizers=(optimizer, scheduler),\n",
    "        )\n",
    "\n",
    "        # Fine tune\n",
    "        finetune_forecast_trainer.train()\n",
    "\n",
    "        # Evaluation\n",
    "        print(\n",
    "            \"+\" * 20,\n",
    "            f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\",\n",
    "            \"+\" * 20,\n",
    "        )\n",
    "        fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
    "        print(fewshot_output)\n",
    "        print(\"+\" * 60)\n",
    "\n",
    "        # Plot\n",
    "        plot_predictions(\n",
    "            finetune_forecast_trainer.model,\n",
    "            dset_test,\n",
    "            SUBDIR,\n",
    "            num_plots=10,\n",
    "            plot_prefix=f\"test_fewshot_{fewshot_percent}\",\n",
    "            channel=0,\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        # write results\n",
    "        all_results[f\"fs{fewshot_percent}_mse\"].append(fewshot_output[\"eval_loss\"])\n",
    "        all_results[f\"fs{fewshot_percent}_mean_epoch_time\"].append(tracking_callback.mean_epoch_time)\n",
    "        all_results[f\"fs{fewshot_percent}_total_train_time\"].append(tracking_callback.total_train_time)\n",
    "        all_results[f\"fs{fewshot_percent}_best_val_metric\"].append(tracking_callback.best_eval_metric)\n",
    "\n",
    "    df_out = pd.DataFrame(all_results).round(3)\n",
    "    print(df_out[[\"dataset\", \"zs_mse\", \"fs5_mse\", \"fs10_mse\"]])\n",
    "    df_out.to_csv(f\"{OUT_DIR}/results_zero_few.csv\")\n",
    "    df_out.to_csv(f\"{OUT_DIR}/results_zero_few.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking results*\n",
    "\n",
    "*Some slight differences in the results as compared to the TTM paper results is possible due to different training environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>zs_mse</th>\n",
       "      <th>fs5_mse</th>\n",
       "      <th>fs10_mse</th>\n",
       "      <th>zs_eval_time</th>\n",
       "      <th>fs5_mean_epoch_time</th>\n",
       "      <th>fs5_total_train_time</th>\n",
       "      <th>fs10_mean_epoch_time</th>\n",
       "      <th>fs10_total_train_time</th>\n",
       "      <th>fs5_best_val_metric</th>\n",
       "      <th>fs10_best_val_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etth1</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.364</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.340</td>\n",
       "      <td>33.769</td>\n",
       "      <td>1.303</td>\n",
       "      <td>49.612</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>etth2</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.435</td>\n",
       "      <td>1.218</td>\n",
       "      <td>32.602</td>\n",
       "      <td>1.303</td>\n",
       "      <td>33.487</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ettm1</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.368</td>\n",
       "      <td>1.727</td>\n",
       "      <td>1.555</td>\n",
       "      <td>72.793</td>\n",
       "      <td>1.930</td>\n",
       "      <td>91.994</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ettm2</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.176</td>\n",
       "      <td>1.673</td>\n",
       "      <td>1.578</td>\n",
       "      <td>51.517</td>\n",
       "      <td>1.951</td>\n",
       "      <td>52.046</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weather</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.149</td>\n",
       "      <td>2.065</td>\n",
       "      <td>1.665</td>\n",
       "      <td>44.358</td>\n",
       "      <td>2.159</td>\n",
       "      <td>50.491</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>electricity</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.139</td>\n",
       "      <td>8.555</td>\n",
       "      <td>3.005</td>\n",
       "      <td>351.540</td>\n",
       "      <td>4.900</td>\n",
       "      <td>275.353</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>traffic</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>14.519</td>\n",
       "      <td>4.312</td>\n",
       "      <td>557.765</td>\n",
       "      <td>7.788</td>\n",
       "      <td>731.283</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  zs_mse  fs5_mse  fs10_mse  zs_eval_time  fs5_mean_epoch_time  \\\n",
       "0        etth1   0.363    0.363     0.364         1.037                1.340   \n",
       "1        etth2   0.286    0.285     0.284         0.435                1.218   \n",
       "2        ettm1   0.415    0.366     0.368         1.727                1.555   \n",
       "3        ettm2   0.186    0.175     0.176         1.673                1.578   \n",
       "4      weather   0.152    0.150     0.149         2.065                1.665   \n",
       "5  electricity   0.170    0.142     0.139         8.555                3.005   \n",
       "6      traffic   0.509    0.397     0.397        14.519                4.312   \n",
       "\n",
       "   fs5_total_train_time  fs10_mean_epoch_time  fs10_total_train_time  \\\n",
       "0                33.769                 1.303                 49.612   \n",
       "1                32.602                 1.303                 33.487   \n",
       "2                72.793                 1.930                 91.994   \n",
       "3                51.517                 1.951                 52.046   \n",
       "4                44.358                 2.159                 50.491   \n",
       "5               351.540                 4.900                275.353   \n",
       "6               557.765                 7.788                731.283   \n",
       "\n",
       "   fs5_best_val_metric  fs10_best_val_metric  \n",
       "0                0.656                 0.654  \n",
       "1                0.208                 0.208  \n",
       "2                0.450                 0.427  \n",
       "3                0.129                 0.129  \n",
       "4                0.423                 0.422  \n",
       "5                0.116                 0.115  \n",
       "6                0.328                 0.331  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
