{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d72278-b1a8-4f28-ab19-66357a411c7e",
   "metadata": {},
   "source": [
    "# Quick Start: Running TTM models on gift-eval benchmark\n",
    "\n",
    "**Tiny Time Mixers (TTMs)** (accepted in NeurIPS 2024) are **compact and lightweight pre-trained models** for time series forecasting, with sizes ranging from **1 to 5 million parameters**. They are designed for **fast fine-tuning** on target domain datasets.  \n",
    "\n",
    "In this script, we demonstrate how to run the **TTM model** on the **GIFT-Eval benchmark** using a **20% few-shot fine-tuning setting**. For more details, see [here](https://github.com/ibm-granite/granite-tsfm/tree/gift/notebooks/hfdemo/tinytimemixer/full_benchmarking/gift_leaderboard). \n",
    "\n",
    "TTM-r2 models have been used in this evaluation. Model card can be found [here](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r2).\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL`\n",
    "environment variable correctly before running this script.\n",
    "We will use the `Dataset` class to load the data and run the model.\n",
    "If you have not already please check out the [dataset.ipynb](./dataset.ipynb)\n",
    "notebook to learn more about the `Dataset` class. We are going to just run\n",
    "the model on two datasets for brevity. But feel free to run on any dataset\n",
    "by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6fef1-714e-4e3b-808a-571b69a4fe73",
   "metadata": {},
   "source": [
    "## TSFM and TTM Installation\n",
    "\n",
    "1. Clone the [GIFT-Eval repository](https://github.com/SalesforceAIResearch/gift-eval).\n",
    "1. Follow the instruction to set up the GIFT-Eval environment as described [here](https://github.com/SalesforceAIResearch/gift-eval?tab=readme-ov-file#installation).\n",
    "1. This notebook should be placed in the `notebooks` folder of the cloned repository.\n",
    "1. Follow the instructions below to install TSFM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad68ca-a9b6-4e68-844d-539037852a99",
   "metadata": {},
   "source": [
    "### Installing `tsfm`\n",
    "\n",
    "The TTM source codes will be installed from the [granite-tsfm repository](https://github.com/ibm-granite/granite-tsfm).\n",
    "Note that `granite-tsfm` installs `pandas==2.2.3` but GIFT-EVAL requires `pandas==2.0.0`.\n",
    "Hence, after installing TTM from `granite-tsfm`, we forece reinstall `pandas==2.0.0`.\n",
    "\n",
    "\n",
    "Run the following code once to install granite-tsfm in your working python environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ecc0baa-2a75-42fc-ae8f-e84afd241130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'granite-tsfm' already exists. Skipping git clone.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists(\"granite-tsfm\"):\n",
    "    !git clone git@github.com:ibm-granite/granite-tsfm.git\n",
    "    %cd granite-tsfm\n",
    "    !pwd\n",
    "    # Switch to the desired branch\n",
    "    !git switch gift\n",
    "    ! pip install \".[notebooks]\"\n",
    "    ! pip install pandas==2.0.0\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"Folder 'granite-tsfm' already exists. Skipping git clone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06614be0-eca8-45bc-91ea-ab9c47bd5c3c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45fd2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Required Imports\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from gift_eval.data import Dataset\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688a5fd-4a0e-48e6-9411-74b34495481c",
   "metadata": {},
   "source": [
    "### Add `TTMGluonTSPredictor` to `PYTHONPATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645eeb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.realpath(\"granite-tsfm/notebooks/hfdemo/tinytimemixer/full_benchmarking/\"))\n",
    "from gift_leaderboard.src.ttm_gluonts_predictor import (\n",
    "    TTM_MAX_FORECAST_HORIZON,\n",
    "    TTMGluonTSPredictor,\n",
    ")\n",
    "from gift_leaderboard.src.utils import get_args, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268c6f5-2220-426c-b915-782649d2f03a",
   "metadata": {},
   "source": [
    "## Set output directory and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa66cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "\n",
    "# Set out dir path\n",
    "OUT_DIR = f\"../results/{args.out_dir}\"\n",
    "\n",
    "# Add arguments\n",
    "SEED = 42\n",
    "\n",
    "# set seed\n",
    "set_seed(SEED)\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df14514-5253-4e9a-b60c-b41dd3c7e11d",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "In[4]:"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "358a12fd",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "## Datasets"
   },
   "outputs": [],
   "source": [
    "# short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "short_datasets = \"us_births/D\"\n",
    "\n",
    "# med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "med_long_datasets = \"ett1/H\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = sorted(set(short_datasets.split() + med_long_datasets.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17ec82-c713-4b23-97ea-d5f8280d69bd",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a5fb2ec",
   "metadata": {
    "title": "## Metrics"
   },
   "outputs": [],
   "source": [
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(forecast_type=\"mean\"),\n",
    "    MAE(forecast_type=0.5),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a593381f-8a6e-4bb4-a267-1969f37420bf",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "Now that we have our predictor class `TTMGluonTSPredictor` imported,\n",
    "we can use it to fine-tune and predict on the gift-eval benchmark datasets.\n",
    "We will use the `train` function to finetune the TTM model, and\n",
    "`evaluate_model` function to evaluate the model.\n",
    "The `evaluate_model` function is a helper function to evaluate the \n",
    "model on the test data and return the results in a dictionary.\n",
    "\n",
    "We are going to follow the naming conventions explained in the\n",
    "[README](../README.md) file to store the results in a csv file\n",
    "called `all_results.csv` under the `results/ttm` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which\n",
    "is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02aeb63-9665-406c-a4ef-d9c03bfe7e23",
   "metadata": {
    "title": "## Evaluation"
   },
   "source": [
    "### Define output file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75fc098",
   "metadata": {
    "title": "Output file"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done datasets\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(OUT_DIR, \"all_results.csv\")\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writerow(\n",
    "            [\n",
    "                \"dataset\",\n",
    "                \"model\",\n",
    "                \"eval_metrics/MSE[mean]\",\n",
    "                \"eval_metrics/MSE[0.5]\",\n",
    "                \"eval_metrics/MAE[mean]\",\n",
    "                \"eval_metrics/MAE[0.5]\",\n",
    "                \"eval_metrics/MASE[0.5]\",\n",
    "                \"eval_metrics/MAPE[0.5]\",\n",
    "                \"eval_metrics/sMAPE[0.5]\",\n",
    "                \"eval_metrics/MSIS\",\n",
    "                \"eval_metrics/RMSE[mean]\",\n",
    "                \"eval_metrics/NRMSE[mean]\",\n",
    "                \"eval_metrics/ND[0.5]\",\n",
    "                \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "                \"domain\",\n",
    "                \"num_variates\",\n",
    "                \"horizon\",\n",
    "                \"ttm_context_len\",\n",
    "                \"available_context_len\",\n",
    "                \"finetune_success\",\n",
    "                \"finetune_train_num_samples\",\n",
    "                \"finetune_valid_num_samples\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "df_res = pd.read_csv(csv_file_path)\n",
    "done_datasets = df_res[\"dataset\"].values\n",
    "print(\"Done datasets\")\n",
    "print(done_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad3e4d-afb4-4785-bf8a-281a22335087",
   "metadata": {},
   "source": [
    "### Run over all defined datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13fc22d7",
   "metadata": {
    "title": "Run over all datasets"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: ett1/H, term: short\n",
      "Dataset: ett1/H, Freq = H, H = 48\n",
      "Minimum context length among all time series in this dataset = 16460\n",
      "prediction_channel_indices = [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n",
      "WARNING:p-1447034:t-23216341889792:get_model.py:get_model:Requested `prediction_length` (48) is not exactly equal to any of the available TTM prediction lengths. Hence, TTM will forecast using the `prediction_filter_length` argument to provide the requested prediction length. Check the model card to know more about the supported context lengths and forecast/prediction lengths.\n",
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:Model loaded successfully from ibm-granite/granite-timeseries-ttm-r2, revision = 1536-96-r2.\n",
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:[TTM] context_length = 1536, prediction_length = 96\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:_get_gift_model:The TTM has Prefix Tuning = False\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Number of series: Train = 1, Valid = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels in the dataset ett1/H = 7\n",
      "Batch size is set to None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 1470.14it/s]\n",
      "1it [00:00, 2205.21it/s]\n",
      "1it [00:00, 2295.73it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Global scaling done successfully.\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Length of orginal train set = 14829\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Length of 20.0 % train set = 2965\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Number of train samples = 2966, valid samples = 11864\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Using a batch size of 64, based on number of training samples = 2966 and number of channels = 7.\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Running learning rate (LR) finder algorithm. If the suggested LR is very low, we suggest setting the LR manually.\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Using GPU:0.\n",
      "/dccstor/dnn_forecasting/conda_envs/envs/gift/lib/python3.10/site-packages/tsfm_public/toolkit/lr_finder.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=device)\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Suggested learning rate = 0.00011768119524349978\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:OPTIMAL SUGGESTED LEARNING RATE = 0.00011768119524349978\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Using learning rate = 0.00011768119524349978\n",
      "WARNING:p-1447034:t-23216341889792:other.py:check_os_kernel:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='940' max='940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [940/940 01:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.370936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.371500</td>\n",
       "      <td>0.364206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.357309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>0.352478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.349900</td>\n",
       "      <td>0.346650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.344139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.340699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.335400</td>\n",
       "      <td>0.339689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>0.337563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.336245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.333112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.332593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>0.331850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.317100</td>\n",
       "      <td>0.329311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.314900</td>\n",
       "      <td>0.328270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.328524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>0.327516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.327234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.327107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.310800</td>\n",
       "      <td>0.327101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrackingCallback] Mean Epoch Time = 1.7376641154289245 seconds, Total Train Time = 85.67431926727295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14830/14830 [00:00<00:00, 130994.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14830/14830 [00:00<00:00, 129645.40it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:get_insample_stats:Successfully, calculated the in-sample statistics.\n",
      "20it [00:00, 10195.20it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b14241688ed4c88aeb53364d177b5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 78398.21it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:predict:Making quantile forecasts for quantiles [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a37f9c5c6b2472289874dc16b103bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 184.41it/s]\n",
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n",
      "WARNING:p-1447034:t-23216341889792:get_model.py:get_model:Requested `prediction_length` (480) is not exactly equal to any of the available TTM prediction lengths. Hence, TTM will forecast using the `prediction_filter_length` argument to provide the requested prediction length. Check the model card to know more about the supported context lengths and forecast/prediction lengths.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/H have been written to ../results/ttm/all_results.csv\n",
      "Processing dataset: ett1/H, term: medium\n",
      "Dataset: ett1/H, Freq = H, H = 480\n",
      "Minimum context length among all time series in this dataset = 15500\n",
      "prediction_channel_indices = [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:Model loaded successfully from ibm-granite/granite-timeseries-ttm-r2, revision = 1536-720-r2.\n",
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:[TTM] context_length = 1536, prediction_length = 720\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:_get_gift_model:The TTM has Prefix Tuning = False\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Number of series: Train = 1, Valid = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels in the dataset ett1/H = 7\n",
      "Batch size is set to None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 1975.65it/s]\n",
      "1it [00:00, 4096.00it/s]\n",
      "1it [00:00, 4419.71it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Global scaling done successfully.\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Length of orginal train set = 13005\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Length of 20.0 % train set = 2601\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Number of train samples = 2602, valid samples = 10404\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Using a batch size of 64, based on number of training samples = 2602 and number of channels = 7.\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Running learning rate (LR) finder algorithm. If the suggested LR is very low, we suggest setting the LR manually.\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Using GPU:0.\n",
      "/dccstor/dnn_forecasting/conda_envs/envs/gift/lib/python3.10/site-packages/tsfm_public/toolkit/lr_finder.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=device)\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Suggested learning rate = 4.641588833612777e-05\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:OPTIMAL SUGGESTED LEARNING RATE = 4.641588833612777e-05\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Using learning rate = 4.641588833612777e-05\n",
      "WARNING:p-1447034:t-23216341889792:other.py:check_os_kernel:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='820' max='820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [820/820 01:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>0.523571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.517600</td>\n",
       "      <td>0.508753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.502100</td>\n",
       "      <td>0.498251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.489026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.482300</td>\n",
       "      <td>0.481359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.472037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.464372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.458610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.454100</td>\n",
       "      <td>0.460955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.450200</td>\n",
       "      <td>0.452701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.453868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.442500</td>\n",
       "      <td>0.447373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>0.447896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.446518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.442734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.444351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.434100</td>\n",
       "      <td>0.444623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>0.442842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.443684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>0.443737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrackingCallback] Mean Epoch Time = 1.5267478227615356 seconds, Total Train Time = 79.7949709892273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13006/13006 [00:00<00:00, 47202.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13006/13006 [00:00<00:00, 50524.98it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:get_insample_stats:Successfully, calculated the in-sample statistics.\n",
      "4it [00:00, 7124.08it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e117c88d04405883fee19819cd1d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 13304.69it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:predict:Making quantile forecasts for quantiles [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8467bd137442d6b622a55f890c8b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 138.62it/s]\n",
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n",
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:Model loaded successfully from ibm-granite/granite-timeseries-ttm-r2, revision = 1536-720-r2.\n",
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:[TTM] context_length = 1536, prediction_length = 720\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:_get_gift_model:The TTM has Prefix Tuning = False\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Number of series: Train = 1, Valid = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/H have been written to ../results/ttm/all_results.csv\n",
      "Processing dataset: ett1/H, term: long\n",
      "Dataset: ett1/H, Freq = H, H = 720\n",
      "Minimum context length among all time series in this dataset = 15260\n",
      "prediction_channel_indices = [0, 1, 2, 3, 4, 5, 6]\n",
      "Number of channels in the dataset ett1/H = 7\n",
      "Batch size is set to None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 3028.38it/s]\n",
      "1it [00:00, 3840.94it/s]\n",
      "1it [00:00, 4056.39it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Global scaling done successfully.\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Length of orginal train set = 12285\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Length of 20.0 % train set = 2457\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Number of train samples = 2458, valid samples = 9828\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Using a batch size of 64, based on number of training samples = 2458 and number of channels = 7.\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Running learning rate (LR) finder algorithm. If the suggested LR is very low, we suggest setting the LR manually.\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Using GPU:0.\n",
      "/dccstor/dnn_forecasting/conda_envs/envs/gift/lib/python3.10/site-packages/tsfm_public/toolkit/lr_finder.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=device)\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Suggested learning rate = 0.00011768119524349978\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:OPTIMAL SUGGESTED LEARNING RATE = 0.00011768119524349978\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Using learning rate = 0.00011768119524349978\n",
      "WARNING:p-1447034:t-23216341889792:other.py:check_os_kernel:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 01:27, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.574800</td>\n",
       "      <td>0.552589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.543400</td>\n",
       "      <td>0.532988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.521800</td>\n",
       "      <td>0.506363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>0.494463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.491257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.475700</td>\n",
       "      <td>0.473564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>0.465527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.456700</td>\n",
       "      <td>0.461155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.455555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.442500</td>\n",
       "      <td>0.450782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>0.446651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>0.438955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.436920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.434880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.433597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.430886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.430882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.430186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.418300</td>\n",
       "      <td>0.430703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.430555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrackingCallback] Mean Epoch Time = 1.6162255525588989 seconds, Total Train Time = 88.64429831504822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12286/12286 [00:00<00:00, 32988.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12286/12286 [00:00<00:00, 38116.72it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:get_insample_stats:Successfully, calculated the in-sample statistics.\n",
      "3it [00:00, 3734.91it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b55da49f3cf41fc91afd11f4390e6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12433.71it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:predict:Making quantile forecasts for quantiles [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a4c93553eb47d2b2dff21180899b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 125.74it/s]\n",
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n",
      "WARNING:p-1447034:t-23216341889792:get_model.py:get_model:Requested `prediction_length` (30) is not exactly equal to any of the available TTM prediction lengths. Hence, TTM will forecast using the `prediction_filter_length` argument to provide the requested prediction length. Check the model card to know more about the supported context lengths and forecast/prediction lengths.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/H have been written to ../results/ttm/all_results.csv\n",
      "Processing dataset: us_births/D, term: short\n",
      "Dataset: us_births/D, Freq = D, H = 30\n",
      "Minimum context length among all time series in this dataset = 6705\n",
      "prediction_channel_indices = [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:Model loaded successfully from ibm-granite/granite-timeseries-ttm-r2, revision = 512-48-ft-l1-r2.1.\n",
      "INFO:p-1447034:t-23216341889792:get_model.py:get_model:[TTM] context_length = 512, prediction_length = 48\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:_get_gift_model:The TTM has Prefix Tuning = True\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Number of series: Train = 1, Valid = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels in the dataset us_births/D = 1\n",
      "Batch size is set to None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 4534.38it/s]\n",
      "1it [00:00, 11335.96it/s]\n",
      "1it [00:00, 11881.88it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Global scaling done successfully.\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Length of orginal train set = 6134\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Length of 20.0 % train set = 1226\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Number of train samples = 1227, valid samples = 4908\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Using a batch size of 64, based on number of training samples = 1227 and number of channels = 1.\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Running learning rate (LR) finder algorithm. If the suggested LR is very low, we suggest setting the LR manually.\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Using GPU:0.\n",
      "/dccstor/dnn_forecasting/conda_envs/envs/gift/lib/python3.10/site-packages/tsfm_public/toolkit/lr_finder.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=device)\n",
      "INFO:p-1447034:t-23216341889792:lr_finder.py:optimal_lr_finder:LR Finder: Suggested learning rate = 0.00017073526474706903\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:OPTIMAL SUGGESTED LEARNING RATE = 0.00017073526474706903\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:train:Using learning rate = 0.00017073526474706903\n",
      "WARNING:p-1447034:t-23216341889792:other.py:check_os_kernel:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 00:31, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.260379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.235483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.227898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.226568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.221158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>0.217749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.214775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.214077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.210030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.207998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.221200</td>\n",
       "      <td>0.203111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>0.204193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>0.201733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.201111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.215200</td>\n",
       "      <td>0.202023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.211500</td>\n",
       "      <td>0.201373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.199580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.199516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.213000</td>\n",
       "      <td>0.199533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.199555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrackingCallback] Mean Epoch Time = 0.7491755962371827 seconds, Total Train Time = 31.291912078857422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6135/6135 [00:00<00:00, 149750.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6135/6135 [00:00<00:00, 131492.63it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:get_insample_stats:Successfully, calculated the in-sample statistics.\n",
      "20it [00:00, 44549.17it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf0233830fb4faeaba5d78ccad67220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 83886.08it/s]\n",
      "INFO:p-1447034:t-23216341889792:ttm_gluonts_predictor.py:predict:Making quantile forecasts for quantiles [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dacff6ec676405d9e1363cabfffd2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 400.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for us_births/D have been written to ../results/ttm/all_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ds_name in all_datasets:\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (term == \"medium\" or term == \"long\") and ds_name not in med_long_datasets.split():\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing dataset: {ds_name}, term: {term}\")\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "        if ds_config in done_datasets:\n",
    "            print(f\"Done with {ds_config}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=False)\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "\n",
    "        print(f\"Dataset: {ds_name}, Freq = {dataset.freq}, H = {dataset.prediction_length}\")\n",
    "\n",
    "        # Get suitable context length for TTM for this dataset\n",
    "        all_lengths = []\n",
    "        for x in dataset.test_data:\n",
    "            if len(x[0][\"target\"].shape) == 1:\n",
    "                all_lengths.append(len(x[0][\"target\"]))\n",
    "                num_channels = 1\n",
    "            else:\n",
    "                all_lengths.append(x[0][\"target\"].shape[1])\n",
    "                num_channels = x[0][\"target\"].shape[0]\n",
    "\n",
    "        min_context_length = min(all_lengths)\n",
    "        print(\n",
    "            \"Minimum context length among all time series in this dataset =\",\n",
    "            min_context_length,\n",
    "        )\n",
    "\n",
    "        # Set channel indices\n",
    "        num_prediction_channels = num_channels\n",
    "        prediction_channel_indices = list(range(num_channels))\n",
    "\n",
    "        # Check existence of \"past_feat_dynamic_real\"\n",
    "        past_feat_dynamic_real_exist = False\n",
    "        if args.use_exogs and \"past_feat_dynamic_real\" in x[0].keys():\n",
    "            num_exogs = x[0][\"past_feat_dynamic_real\"].shape[0]\n",
    "            print(f\"Data has `past_feat_dynamic_real` features of size {num_exogs}.\")\n",
    "            num_channels += num_exogs\n",
    "            past_feat_dynamic_real_exist = True\n",
    "\n",
    "        if dataset.prediction_length > TTM_MAX_FORECAST_HORIZON:\n",
    "            # predict all channels, needed for recursive forecast\n",
    "            prediction_channel_indices = list(range(num_channels))\n",
    "\n",
    "        print(\"prediction_channel_indices =\", prediction_channel_indices)\n",
    "\n",
    "        # For very short series, force short context window creatiio for finetuning\n",
    "        if term == \"short\":\n",
    "            force_short_context = args.force_short_context\n",
    "        else:\n",
    "            force_short_context = False\n",
    "\n",
    "        # Instantiate the TTM GluonTS Predictor with the minimum context length in the dataset\n",
    "        # The predictor will automatically choose the suitable context and forecast length\n",
    "        # of the TTM model.\n",
    "        predictor = TTMGluonTSPredictor(\n",
    "            context_length=min_context_length,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "            model_path=args.model_path,\n",
    "            test_data_label=dataset.test_data.label,\n",
    "            random_seed=SEED,\n",
    "            term=term,\n",
    "            ds_name=ds_name,\n",
    "            out_dir=OUT_DIR,\n",
    "            scale=True,\n",
    "            upper_bound_fewshot_samples=args.upper_bound_fewshot_samples,\n",
    "            force_short_context=force_short_context,\n",
    "            min_context_mult=args.min_context_mult,\n",
    "            past_feat_dynamic_real_exist=past_feat_dynamic_real_exist,\n",
    "            num_prediction_channels=num_prediction_channels,\n",
    "            freq=dataset.freq,\n",
    "            use_valid_from_train=args.use_valid_from_train,\n",
    "            insample_forecast=args.insample_forecast,\n",
    "            insample_use_train=args.insample_use_train,\n",
    "            # TTM kwargs\n",
    "            head_dropout=args.head_dropout,\n",
    "            decoder_mode=args.decoder_mode,\n",
    "            num_input_channels=num_channels,\n",
    "            huber_delta=args.huber_delta,\n",
    "            quantile=args.quantile,\n",
    "            loss=args.loss,\n",
    "            prediction_channel_indices=prediction_channel_indices,\n",
    "        )\n",
    "\n",
    "        print(f\"Number of channels in the dataset {ds_name} =\", num_channels)\n",
    "        if args.batch_size is None:\n",
    "            batch_size = None\n",
    "            optimize_batch_size = True\n",
    "        else:\n",
    "            batch_size = args.batch_size\n",
    "            optimize_batch_size = False\n",
    "        print(\"Batch size is set to\", batch_size)\n",
    "\n",
    "        finetune_train_num_samples = 0\n",
    "        finetune_valid_num_samples = 0\n",
    "        try:\n",
    "            # finetune the model on the train split\n",
    "            predictor.train(\n",
    "                train_dataset=dataset.training_dataset,\n",
    "                valid_dataset=dataset.validation_dataset,\n",
    "                batch_size=batch_size,\n",
    "                optimize_batch_size=optimize_batch_size,\n",
    "                freeze_backbone=args.freeze_backbone,\n",
    "                learning_rate=args.learning_rate,\n",
    "                num_epochs=args.num_epochs,\n",
    "                fewshot_fraction=args.fewshot_fraction,\n",
    "                fewshot_location=args.fewshot_location,\n",
    "                automate_fewshot_fraction=args.automate_fewshot_fraction,\n",
    "                automate_fewshot_fraction_threshold=args.automate_fewshot_fraction_threshold,\n",
    "            )\n",
    "            finetune_success = True\n",
    "            finetune_train_num_samples = predictor.train_num_samples\n",
    "            finetune_valid_num_samples = predictor.valid_num_samples\n",
    "        except Exception as e:\n",
    "            print(\"Error in finetune workflow. Error =\", e)\n",
    "            print(\"Fallback to zero-shot performance.\")\n",
    "            finetune_success = False\n",
    "\n",
    "        # Evaluate\n",
    "        res = evaluate_model(\n",
    "            predictor,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            batch_size=batch_size,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        )\n",
    "\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    \"TTM\",\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[mean]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    dataset_properties_map[ds_key][\"domain\"],\n",
    "                    dataset_properties_map[ds_key][\"num_variates\"],\n",
    "                    dataset.prediction_length,\n",
    "                    predictor.ttm.config.context_length,\n",
    "                    min_context_length,\n",
    "                    finetune_success,\n",
    "                    finetune_train_num_samples,\n",
    "                    finetune_valid_num_samples,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffd629a1",
   "metadata": {
    "title": "Save results"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>eval_metrics/MASE[0.5]</th>\n",
       "      <th>eval_metrics/NRMSE[mean]</th>\n",
       "      <th>eval_metrics/mean_weighted_sum_quantile_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ett1/H/long</td>\n",
       "      <td>1.391650</td>\n",
       "      <td>0.565134</td>\n",
       "      <td>0.281423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ett1/H/medium</td>\n",
       "      <td>1.283772</td>\n",
       "      <td>0.559106</td>\n",
       "      <td>0.273191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ett1/H/short</td>\n",
       "      <td>0.849190</td>\n",
       "      <td>0.456671</td>\n",
       "      <td>0.195602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us_births/D/short</td>\n",
       "      <td>0.382331</td>\n",
       "      <td>0.040773</td>\n",
       "      <td>0.020100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataset  eval_metrics/MASE[0.5]  eval_metrics/NRMSE[mean]   \n",
       "2        ett1/H/long                1.391650                  0.565134  \\\n",
       "1      ett1/H/medium                1.283772                  0.559106   \n",
       "0       ett1/H/short                0.849190                  0.456671   \n",
       "3  us_births/D/short                0.382331                  0.040773   \n",
       "\n",
       "   eval_metrics/mean_weighted_sum_quantile_loss  \n",
       "2                                      0.281423  \n",
       "1                                      0.273191  \n",
       "0                                      0.195602  \n",
       "3                                      0.020100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results\n",
    "df = pd.read_csv(f\"{OUT_DIR}/all_results.csv\")\n",
    "df = df.sort_values(by=\"dataset\")\n",
    "display(\n",
    "    df[\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee73d8-9843-4d25-9aae-58fab8534277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
