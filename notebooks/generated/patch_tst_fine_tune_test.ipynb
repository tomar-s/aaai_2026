{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # Channel Independence Patch Time Series Transformer\n",
    " Fine tuning for forecasting\n",
    "\n",
    " Maybe add a picture of the PatchTST with forecasting head?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tsfmservices.toolkit.dataset import ForecastDFDataset\n",
    "from transformers import (\n",
    "    PatchTSTConfig,\n",
    "    PatchTSTForForecasting,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Load and prepare datasets\n",
    "\n",
    " Please adjust the following parameters to suit your application:\n",
    " - timestamp_column: column name containing timestamp information, use None if there is no such column\n",
    " - id_columns: List of column names specifying the IDs of different time series. If no ID column exists, use []\n",
    " - forecast_columns: List of columns to be modeled\n",
    " - context_length: Specifies how many historical time points are used by the model\n",
    " - prediction_length: Specifies how many timepoints should be forecasted\n",
    "\n",
    " Using the parameters above load the data, divide it into train and eval portions, and create torch datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "timestamp_column = \"date\"\n",
    "id_columns = []\n",
    "forecast_columns = [\"OT\"]\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\",\n",
    "    parse_dates=[timestamp_column],\n",
    ")\n",
    "print(data.head())\n",
    "\n",
    "pretrained_model_path = \"model/pretrained\"\n",
    "pretrained_config = PatchTSTConfig.from_pretrained(pretrained_model_path)\n",
    "\n",
    "prediction_length = 20\n",
    "context_length = 32  # use pretrained_config.context_length to match pretrained model\n",
    "\n",
    "# to do: split data\n",
    "# need utility here, group sensitive splitting should be done\n",
    "train_data = data.iloc[: 12 * 30 * 24,].copy()\n",
    "eval_data = data.iloc[\n",
    "    12 * 30 * 24 - context_length : 12 * 30 * 24 + 4 * 30 * 24,\n",
    "].copy()\n",
    "\n",
    "\n",
    "train_dataset = ForecastDFDataset(\n",
    "    train_data,\n",
    "    timestamp_column=timestamp_column,\n",
    "    id_columns=id_columns,\n",
    "    input_columns=forecast_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    ")\n",
    "eval_dataset = ForecastDFDataset(\n",
    "    eval_data,\n",
    "    timestamp_column=timestamp_column,\n",
    "    id_columns=id_columns,\n",
    "    input_columns=forecast_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
      "0 2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
      "1 2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
      "2 2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
      "3 2016-07-01 03:00:00  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
      "4 2016-07-01 04:00:00  5.358  1.942  1.492  0.462  3.868  1.279  21.948000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Configure the PatchTST model\n",
    "\n",
    " Describe only forecasting specific parameters that are configurable here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "pred_config = PatchTSTConfig.from_pretrained(\n",
    "    pretrained_model_path,\n",
    "    context_length=context_length,\n",
    "    num_input_channels=len(forecast_columns),\n",
    "    prediction_length=prediction_length,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Load model and freeze base model parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "forecasting_model = PatchTSTForForecasting.from_pretrained(\n",
    "    \"model/pretrained\",\n",
    "    config=pred_config,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "# to unfreeze the base model parameters, comment out the cell\n",
    "for param in forecasting_model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of PatchTSTForForecasting were not initialized from the model checkpoint at model/pretrained and are newly initialized because the shapes did not match:\n",
      "- head.linear.weight: found shape torch.Size([12, 16]) in the checkpoint and torch.Size([20, 16]) in the model instantiated\n",
      "- head.linear.bias: found shape torch.Size([12]) in the checkpoint and torch.Size([20]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Train model\n",
    " Provide description of important training parameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoint/forecast\",\n",
    "    # logging_steps = 100,\n",
    "    # per_device_train_batch_size = 64, #defaults to 8\n",
    "    # per_device_eval_batch_size = 64, #defaults to 8\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # eval_steps = 100,\n",
    "    save_total_limit=5,\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    max_steps=10,  # For a quick test\n",
    "    label_names=[\"future_values\"],\n",
    ")\n",
    "\n",
    "\n",
    "forecasting_trainer = Trainer(\n",
    "    model=forecasting_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "forecasting_trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15a262ed894a4504b64ab1292b755605"
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/wmgifford/opt/miniconda3/envs/test_ogv/lib/python3.10/site-packages/transformers/models/patchtst/modeling_patchtst.py:299: UserWarning: torch.sort is supported by MPS on MacOS 13+, please upgrade. Falling back to CPU (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Sort.mm:30.)\n",
      "  ids_shuffle = torch.argsort(noise, dim=-1)  # ascend: small is keep, large is remove\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 12.4689, 'learning_rate': 0.0, 'epoch': 0.01}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9f56c21cbc74451a5ff364b3c80472d"
      },
      "text/plain": [
       "  0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 6.40577507019043, 'eval_runtime': 9.021, 'eval_samples_per_second': 317.148, 'eval_steps_per_second': 39.685, 'epoch': 0.01}\n",
      "{'train_runtime': 10.0074, 'train_samples_per_second': 7.994, 'train_steps_per_second': 0.999, 'train_loss': 12.468918609619141, 'epoch': 0.01}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=12.468918609619141, metrics={'train_runtime': 10.0074, 'train_samples_per_second': 7.994, 'train_steps_per_second': 0.999, 'train_loss': 12.468918609619141, 'epoch': 0.01})"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# ## Inference\n",
    "#\n",
    "# To do: use pipeline code to produce more friendly output\n",
    "import torch, copy\n",
    "\n",
    "device = forecasting_model.device\n",
    "\n",
    "\n",
    "data_sample = copy.copy(eval_dataset[0])\n",
    "data_sample[\"past_values\"] = torch.unsqueeze(data_sample[\"past_values\"], 0)\n",
    "data_sample[\"future_values\"] = torch.unsqueeze(data_sample[\"future_values\"], 0)\n",
    "forecasting_model(\n",
    "    data_sample[\"past_values\"].to(device),\n",
    "    future_values=data_sample[\"future_values\"].to(device),\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/wmgifford/opt/miniconda3/envs/test_ogv/lib/python3.10/site-packages/torch/_tensor_str.py:115: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:218.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PatchTSTForForecastingOutput(loss=tensor(2.2564, device='mps:0', grad_fn=<MseLossBackward0>), forecast_outputs=tensor([[[21.2610],\n",
       "         [21.4202],\n",
       "         [21.3384],\n",
       "         [21.3653],\n",
       "         [21.4324],\n",
       "         [21.4399],\n",
       "         [21.4298],\n",
       "         [21.2684],\n",
       "         [21.3639],\n",
       "         [21.4728],\n",
       "         [21.3428],\n",
       "         [21.2446],\n",
       "         [21.3248],\n",
       "         [21.3429],\n",
       "         [21.3274],\n",
       "         [21.3319],\n",
       "         [21.2655],\n",
       "         [21.4246],\n",
       "         [21.2654],\n",
       "         [21.3365]]], device='mps:0', grad_fn=<AddBackward0>), hidden_states=[], attentions=None)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}