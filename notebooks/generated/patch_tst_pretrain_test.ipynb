{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Channel Independence Patch Time Series Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Channel Independence PatchTST model - an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning - is demonstrated in the following diagram. It is based on two key components:\n",
    "\n",
    "   - segmentation of time series into subseries-level patches which are served as input tokens to Transformer;\n",
    "\n",
    "   - channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series.\n",
    "\n",
    " Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history.\n",
    "\n",
    " Channel independence allows each time series to have its own embedding and attention maps while sharing the same model parameters across different channels.\n",
    "\n",
    " <div> <img src=\"./figures/patchTST.png\" alt=\"Drawing\" style=\"width: 600px;\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tsfmservices.toolkit.dataset import PretrainDFDataset\n",
    "from transformers import (\n",
    "    PatchTSTConfig,\n",
    "    PatchTSTForMaskPretraining,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load and prepare datasets\n",
    "\n",
    " For the dataset of interest, you must supply the names of the following columns:\n",
    "\n",
    " - timestamp_column: column name containing timestamp information, use None if there is no such column\n",
    " - id_columns: List of column names specifying the IDs of different time series. If no ID column exists, use []\n",
    " - forecast_columns: List of columns to be modeled\n",
    "\n",
    " The data is first loaded into a Pandas dataframe and then converted the appropriate torch dataset needed\n",
    " for training. Finaly, the data is split into training and evaluation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
      "0 2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
      "1 2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
      "2 2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
      "3 2016-07-01 03:00:00  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
      "4 2016-07-01 04:00:00  5.358  1.942  1.492  0.462  3.868  1.279  21.948000\n"
     ]
    }
   ],
   "source": [
    "timestamp_column = \"date\"\n",
    "id_columns = []\n",
    "forecast_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\"]\n",
    "\n",
    "context_length = 16  # 512\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\",\n",
    "    parse_dates=[timestamp_column],\n",
    ")\n",
    "print(data.head())\n",
    "\n",
    "# to do: split data\n",
    "# need utility here, group sensitive splitting should be done\n",
    "train_data = data.iloc[: 12 * 30 * 24,].copy()\n",
    "eval_data = data.iloc[\n",
    "    12 * 30 * 24 - context_length : 12 * 30 * 24 + 4 * 30 * 24,\n",
    "].copy()\n",
    "\n",
    "\n",
    "train_dataset = PretrainDFDataset(\n",
    "    train_data,\n",
    "    timestamp_column=timestamp_column,\n",
    "    id_columns=id_columns,\n",
    "    input_columns=forecast_columns,\n",
    "    context_length=context_length,\n",
    ")\n",
    "eval_dataset = PretrainDFDataset(\n",
    "    eval_data,\n",
    "    timestamp_column=timestamp_column,\n",
    "    id_columns=id_columns,\n",
    "    input_columns=forecast_columns,\n",
    "    context_length=context_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Configure the PatchTST model\n",
    "\n",
    " Describe parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PatchTSTConfig(\n",
    "    num_input_channels=len(forecast_columns),\n",
    "    context_length=context_length,\n",
    "    patch_length=12,\n",
    "    stride=12,\n",
    "    mask_ratio=0.4,\n",
    "    # standardscale=None,  # 'bysample'\n",
    "    d_model=16,  # 128,\n",
    "    encoder_attention_heads=2,  # 16,\n",
    "    encoder_layers=2,  # 6,\n",
    "    encoder_ffn_dim=16,  # 512,\n",
    "    dropout=0.2,\n",
    "    head_dropout=0.2,\n",
    ")\n",
    "pretraining_model = PatchTSTForMaskPretraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wmgifford/opt/miniconda3/envs/test_ogv/lib/python3.10/site-packages/transformers/training_args.py:1812: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers.`mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "/Users/wmgifford/opt/miniconda3/envs/test_ogv/lib/python3.10/site-packages/transformers/models/patchtst/modeling_patchtst.py:299: UserWarning: torch.sort is supported by MPS on MacOS 13+, please upgrade. Falling back to CPU (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Sort.mm:30.)\n",
      "  ids_shuffle = torch.argsort(noise, dim=-1)  # ascend: small is keep, large is remove\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.929200</td>\n",
       "      <td>0.917777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.9291750907897949, metrics={'train_runtime': 11.1571, 'train_samples_per_second': 7.17, 'train_steps_per_second': 0.896, 'total_flos': 176025600.0, 'train_loss': 0.9291750907897949, 'epoch': 0.01})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoint/pretrain\",\n",
    "    # logging_steps = 100,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # eval_steps = 100,\n",
    "    save_total_limit=5,\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    use_mps_device=True,\n",
    "    max_steps=10,\n",
    "    label_names=[\"past_values\"],\n",
    ")\n",
    "pretrainer = Trainer(\n",
    "    model=pretraining_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "pretrainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainer.save_model(\"model/pretrained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
